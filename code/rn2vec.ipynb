{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use all ingredients in recipes as ingredient nodes, instead of only detected ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 16 22:07:55 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.80       Driver Version: 460.80       CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:02:00.0 Off |                    0 |\r\n",
      "| N/A   31C    P0    28W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:03:00.0 Off |                    0 |\r\n",
      "| N/A   27C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla P100-PCIE...  Off  | 00000000:82:00.0 Off |                    0 |\r\n",
      "| N/A   29C    P0    29W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla P100-PCIE...  Off  | 00000000:83:00.0 Off |                    0 |\r\n",
      "| N/A   28C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!module load conda cudnn cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import random\n",
    "import heapq\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import lmdb\n",
    "import gensim\n",
    "\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchfile\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build graph - Small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = '../data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of recipes_subset:  77733\n",
      "train/val/test split:  Counter({'train': 54358, 'test': 11759, 'val': 11616})\n"
     ]
    }
   ],
   "source": [
    "# recipes_subset = json.load(open(dataset_folder+'/recipes_weighted_and_USDAmapped.json', 'r'))\n",
    "# print('length of recipes_subset: ', len(recipes_subset))\n",
    "\n",
    "# partition_list = []\n",
    "# for recipe in recipes_subset:\n",
    "#     partition = recipe['partition']\n",
    "#     partition_list.append(partition)\n",
    "# print('train/val/test split: ', Counter(partition_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating graph ...\n",
      "graph:  Graph(num_nodes={'ingredient': 9271, 'recipe': 77733},\n",
      "      num_edges={('ingredient', 'i-i', 'ingredient'): 148168, ('ingredient', 'i-r', 'recipe'): 524671, ('recipe', 'r-i', 'ingredient'): 524671, ('recipe', 'r-r', 'recipe'): 850354},\n",
      "      metagraph=[('ingredient', 'ingredient', 'i-i'), ('ingredient', 'recipe', 'i-r'), ('recipe', 'ingredient', 'r-i'), ('recipe', 'recipe', 'r-r')])\n"
     ]
    }
   ],
   "source": [
    "def get_graph():\n",
    "    print('generating graph ...')\n",
    "    edge_src, edge_dst, r_i_edge_weight = torch.load(dataset_folder+'/edge_src_and_edge_dst_and_r_i_edge_weight.pt')\n",
    "    recipe_edge_src, recipe_edge_dst, recipe_edge_weight = torch.load(dataset_folder+'/recipe_edge_src_and_recipe_edge_dst_and_weight.pt')\n",
    "    ingre_edge_src, ingre_edge_dst, ingre_edge_weight = torch.load(dataset_folder+'/ingre_edge_src_and_dst_and_weight.pt')\n",
    "\n",
    "    graph = dgl.heterograph({\n",
    "        ('recipe', 'r-i', 'ingredient'): (edge_src, edge_dst),\n",
    "        ('ingredient', 'i-r', 'recipe'): (edge_dst, edge_src),\n",
    "        ('recipe', 'r-r', 'recipe'): (recipe_edge_src, recipe_edge_dst),\n",
    "        ('ingredient', 'i-i', 'ingredient'): (ingre_edge_src, ingre_edge_dst)\n",
    "    })\n",
    "\n",
    "    graph.edges['r-i'].data['weight'] = torch.FloatTensor(r_i_edge_weight)\n",
    "    graph.edges['i-r'].data['weight'] = torch.FloatTensor(r_i_edge_weight)\n",
    "    graph.edges['r-r'].data['weight'] = torch.FloatTensor(recipe_edge_weight)\n",
    "    graph.edges['i-i'].data['weight'] = torch.FloatTensor(ingre_edge_weight)\n",
    "    \n",
    "    recipe_nodes_instruction_features = torch.load(dataset_folder+'recipe_nodes_instruction_features.pt')\n",
    "    ingredient_nodes_nutrient_features_minus1 = torch.load(dataset_folder+'/ingredient_nodes_nutrient_features.pt')\n",
    "    train_mask = torch.load(dataset_folder+'/recipe_nodes_train_mask.pt')\n",
    "    val_mask = torch.load(dataset_folder+'/recipe_nodes_val_mask.pt')\n",
    "    test_mask = torch.load(dataset_folder+'/recipe_nodes_test_mask.pt')\n",
    "    recipe_nodes_labels = torch.load(dataset_folder+'/recipe_nodes_labels.pt')\n",
    "\n",
    "    graph.nodes['recipe'].data['instr_feature'] = recipe_nodes_instruction_features\n",
    "    graph.nodes['ingredient'].data['nutrient_feature'] = ingredient_nodes_nutrient_features_minus1\n",
    "    graph.nodes['recipe'].data['train_mask'] = train_mask\n",
    "    graph.nodes['recipe'].data['val_mask'] = val_mask\n",
    "    graph.nodes['recipe'].data['test_mask'] = test_mask\n",
    "    graph.nodes['recipe'].data['label'] = recipe_nodes_labels.long()\n",
    "    \n",
    "    return graph\n",
    "\n",
    "graph = get_graph()\n",
    "graph.to(device)\n",
    "print('graph: ', graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mask:  torch.Size([77733])\n",
      "val_mask:  torch.Size([77733])\n",
      "test_mask:  torch.Size([77733])\n",
      "labels:  torch.Size([77733])\n"
     ]
    }
   ],
   "source": [
    "# get train/val/test mask\n",
    "train_mask = graph.nodes['recipe'].data['train_mask'].to(device)\n",
    "val_mask = graph.nodes['recipe'].data['val_mask'].to(device)\n",
    "test_mask = graph.nodes['recipe'].data['test_mask'].to(device)\n",
    "labels = graph.nodes['recipe'].data['label'].to(device)\n",
    "\n",
    "print('train_mask: ', train_mask.size())\n",
    "print('val_mask: ', val_mask.size())\n",
    "print('test_mask: ', test_mask.size())\n",
    "print('labels: ', labels.size())\n",
    "\n",
    "train_idx = torch.nonzero(train_mask, as_tuple=False).squeeze()\n",
    "val_idx = torch.nonzero(val_mask, as_tuple=False).squeeze()\n",
    "test_idx = torch.nonzero(test_mask, as_tuple=False).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77733/77733 [00:19<00:00, 3932.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ingre_neighbor_tensor:  torch.Size([77733, 33])\n",
      "ingre_length_tensor:  torch.Size([77733])\n",
      "total_length_index_list:  77734\n",
      "total_ingre_neighbor_tensor:  torch.Size([515189])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get ingre neighbors for each recipe nodes\n",
    "def get_recipe2ingreNeighbor_dict():\n",
    "    max_length = 33\n",
    "    # print(max(len(x) for x in neighbor_list))\n",
    "    out = {}\n",
    "    neighbor_list = []\n",
    "    ingre_length_list = []\n",
    "    total_length_index_list = []\n",
    "    total_ingre_neighbor_list = []\n",
    "    total_length_index = 0\n",
    "    total_length_index_list.append(total_length_index)\n",
    "    for recipeNodeID in tqdm(range(graph.number_of_nodes('recipe'))):\n",
    "        _, succs = graph.out_edges(recipeNodeID, etype='r-i')\n",
    "        succs_list = list(set(succs.tolist()))\n",
    "        total_ingre_neighbor_list.extend(succs_list)\n",
    "        cur_length = len(succs_list)\n",
    "        ingre_length_list.append(cur_length)\n",
    "        \n",
    "        total_length_index += cur_length\n",
    "        total_length_index_list.append(total_length_index)\n",
    "        while len(succs_list) < max_length:\n",
    "            succs_list.append(77733)\n",
    "        neighbor_list.append(succs_list)\n",
    "\n",
    "    ingre_neighbor_tensor = torch.tensor(neighbor_list)\n",
    "    ingre_length_tensor = torch.tensor(ingre_length_list)\n",
    "    total_ingre_neighbor_tensor = torch.tensor(total_ingre_neighbor_list)\n",
    "    return ingre_neighbor_tensor, ingre_length_tensor, total_length_index_list, total_ingre_neighbor_tensor\n",
    "\n",
    "ingre_neighbor_tensor, ingre_length_tensor, total_length_index_list, total_ingre_neighbor_tensor = get_recipe2ingreNeighbor_dict()\n",
    "print('ingre_neighbor_tensor: ', ingre_neighbor_tensor.shape)\n",
    "print('ingre_length_tensor: ', ingre_length_tensor.shape)\n",
    "print('total_length_index_list: ', len(total_length_index_list))\n",
    "print('total_ingre_neighbor_tensor: ', total_ingre_neighbor_tensor.shape)\n",
    "\n",
    "def find(tensor, values):\n",
    "    return torch.nonzero(tensor[..., None] == values)\n",
    "\n",
    "# example for find()\n",
    "# a = torch.tensor([0, 10, 20, 30])\n",
    "# b = torch.tensor([[ 0, 30, 20,  10, 77733],[ 0, 30, 20,  10, 77733]])\n",
    "# find(b, a)[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ingredient_neighbors_link_scores(blocks, output_nodes, secondToLast_ingre, recipe):\n",
    "    ingreNodeIDs = blocks[1].srcdata['_ID']['ingredient']\n",
    "    recipeNodeIDs = output_nodes['recipe']\n",
    "    batch_ingre_neighbors = ingre_neighbor_tensor[recipeNodeIDs].to(device)\n",
    "    batch_ingre_length = ingre_length_tensor[recipeNodeIDs]\n",
    "    valid_batch_ingre_neighbors = find(batch_ingre_neighbors, ingreNodeIDs)[:, 2]\n",
    "\n",
    "    # based on valid_batch_ingre_neighbors each row index\n",
    "    _, valid_batch_ingre_length = torch.unique(find(batch_ingre_neighbors, ingreNodeIDs)[:, 0], return_counts=True)\n",
    "    batch_sum_ingre_length = np.cumsum(valid_batch_ingre_length.cpu())\n",
    "\n",
    "    total_ingre_emb = None\n",
    "    total_pos_score = None\n",
    "    total_neg_score = None\n",
    "\n",
    "    for i in range(len(recipeNodeIDs)):\n",
    "        if i == 0:\n",
    "            recipeNode_ingres = valid_batch_ingre_neighbors[0:batch_sum_ingre_length[i]]\n",
    "            potential_neg_ingres = valid_batch_ingre_neighbors[batch_sum_ingre_length[i]:]\n",
    "            neg_ingres = potential_neg_ingres[torch.randint(len(potential_neg_ingres), (len(recipeNode_ingres),))]\n",
    "            a = secondToLast_ingre[recipeNode_ingres]\n",
    "            b = secondToLast_ingre[neg_ingres]\n",
    "        else:\n",
    "            recipeNode_ingres = valid_batch_ingre_neighbors[batch_sum_ingre_length[i-1]:batch_sum_ingre_length[i]]\n",
    "            potential_neg_ingres = torch.cat([valid_batch_ingre_neighbors[:batch_sum_ingre_length[i-1]], valid_batch_ingre_neighbors[batch_sum_ingre_length[i]:]])\n",
    "            neg_ingres = potential_neg_ingres[torch.randint(len(potential_neg_ingres), (len(recipeNode_ingres),))]\n",
    "            a = secondToLast_ingre[recipeNode_ingres]\n",
    "            b = secondToLast_ingre[neg_ingres]\n",
    "        \n",
    "        cur_recipe = recipe[i,:]\n",
    "        pos_score = torch.mm(a, cur_recipe.unsqueeze(1))\n",
    "        neg_score = torch.mm(b, cur_recipe.unsqueeze(1))\n",
    "        \n",
    "        if total_pos_score == None:\n",
    "            total_pos_score = pos_score\n",
    "            total_neg_score = neg_score\n",
    "        else:\n",
    "            total_pos_score = torch.cat([total_pos_score, pos_score], dim = 0)\n",
    "            total_neg_score = torch.cat([total_neg_score, neg_score], dim = 0)\n",
    "\n",
    "    total_pos_score = total_pos_score.squeeze()\n",
    "    total_neg_score = total_neg_score.squeeze()\n",
    "\n",
    "    return total_pos_score, total_neg_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ingredient_neighbors_all_embeddings(blocks, output_nodes, secondToLast_ingre):\n",
    "    ingreNodeIDs = blocks[1].srcdata['_ID']['ingredient']\n",
    "    recipeNodeIDs = output_nodes['recipe']\n",
    "    batch_ingre_neighbors = ingre_neighbor_tensor[recipeNodeIDs].to(device)\n",
    "    batch_ingre_length = ingre_length_tensor[recipeNodeIDs]\n",
    "    valid_batch_ingre_neighbors = find(batch_ingre_neighbors, ingreNodeIDs)[:, 2]\n",
    "    \n",
    "    # based on valid_batch_ingre_neighbors each row index\n",
    "    _, valid_batch_ingre_length = torch.unique(find(batch_ingre_neighbors, ingreNodeIDs)[:, 0], return_counts=True)\n",
    "    batch_sum_ingre_length = np.cumsum(valid_batch_ingre_length.cpu())\n",
    "\n",
    "    total_ingre_emb = None\n",
    "    for i in range(len(recipeNodeIDs)):\n",
    "        if i == 0:\n",
    "            recipeNode_ingres = valid_batch_ingre_neighbors[0:batch_sum_ingre_length[i]]\n",
    "            a = secondToLast_ingre[recipeNode_ingres]\n",
    "        else:\n",
    "            recipeNode_ingres = valid_batch_ingre_neighbors[batch_sum_ingre_length[i-1]:batch_sum_ingre_length[i]]\n",
    "            a = secondToLast_ingre[recipeNode_ingres]\n",
    "    \n",
    "        # all ingre instead of average\n",
    "        a_rows = a.shape[0]\n",
    "        a_columns = a.shape[1]\n",
    "        max_rows = 5\n",
    "        if a_rows < max_rows:\n",
    "            a = torch.cat([a, torch.zeros(max_rows-a_rows, a_columns).cuda()])\n",
    "        else:\n",
    "            a = a[:max_rows, :]\n",
    "        \n",
    "        if total_ingre_emb == None:\n",
    "            total_ingre_emb = a.unsqueeze(0)\n",
    "        else:\n",
    "            total_ingre_emb = torch.cat([total_ingre_emb,a.unsqueeze(0)], dim = 0)\n",
    "            if torch.isnan(total_ingre_emb).any():\n",
    "                print('Error!')\n",
    "\n",
    "    return total_ingre_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Scaled Dot-Product Attention.\"\"\"\n",
    "\n",
    "    def __init__(self, temperature):\n",
    "        super().__init__()\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"\n",
    "        It is equivariant to permutations\n",
    "        of the batch dimension (`b`).\n",
    "\n",
    "        It is equivariant to permutations of the\n",
    "        second dimension of the queries (`n`).\n",
    "\n",
    "        It is invariant to permutations of the\n",
    "        second dimension of keys and values (`m`).\n",
    "\n",
    "        Arguments:\n",
    "            queries: a float tensor with shape [b, n, d].\n",
    "            keys: a float tensor with shape [b, m, d].\n",
    "            values: a float tensor with shape [b, m, d'].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d'].\n",
    "        \"\"\"\n",
    "\n",
    "        attention = torch.bmm(queries, keys.transpose(1, 2))\n",
    "        attention = self.softmax(attention / self.temperature)\n",
    "        # it has shape [b, n, m]\n",
    "\n",
    "        return torch.bmm(attention, values)\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d, h):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            d: an integer, dimension of queries and values.\n",
    "                It is assumed that input and\n",
    "                output dimensions are the same.\n",
    "            h: an integer, number of heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert d % h == 0\n",
    "        self.h = h\n",
    "\n",
    "        # everything is projected to this dimension\n",
    "        p = d // h\n",
    "\n",
    "        self.project_queries = nn.Linear(d, d)\n",
    "        self.project_keys = nn.Linear(d, d)\n",
    "        self.project_values = nn.Linear(d, d)\n",
    "        self.concatenation = nn.Linear(d, d)\n",
    "        self.attention = Attention(temperature=p**0.5)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            queries: a float tensor with shape [b, n, d].\n",
    "            keys: a float tensor with shape [b, m, d].\n",
    "            values: a float tensor with shape [b, m, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "\n",
    "        h = self.h\n",
    "        b, n, d = queries.size()\n",
    "        _, m, _ = keys.size()\n",
    "        p = d // h\n",
    "\n",
    "        queries = self.project_queries(queries)  # shape [b, n, d]\n",
    "        keys = self.project_keys(keys)  # shape [b, m, d]\n",
    "        values = self.project_values(values)  # shape [b, m, d]\n",
    "\n",
    "        queries = queries.view(b, n, h, p)\n",
    "        keys = keys.view(b, m, h, p)\n",
    "        values = values.view(b, m, h, p)\n",
    "\n",
    "        queries = queries.permute(2, 0, 1, 3).contiguous().view(h * b, n, p)\n",
    "        keys = keys.permute(2, 0, 1, 3).contiguous().view(h * b, m, p)\n",
    "        values = values.permute(2, 0, 1, 3).contiguous().view(h * b, m, p)\n",
    "\n",
    "        output = self.attention(queries, keys, values)  # shape [h * b, n, p]\n",
    "        output = output.view(h, b, n, p)\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(b, n, d)\n",
    "        output = self.concatenation(output)  # shape [b, n, d]\n",
    "\n",
    "        return output\n",
    "\n",
    "class RFF(nn.Module):\n",
    "    \"\"\"\n",
    "    Row-wise FeedForward layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(d, d), nn.ReLU(inplace=True),\n",
    "            nn.Linear(d, d), nn.ReLU(inplace=True),\n",
    "            nn.Linear(d, d), nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "class MultiheadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d, h, rff):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            d: an integer, input dimension.\n",
    "            h: an integer, number of heads.\n",
    "            rff: a module, row-wise feedforward layers.\n",
    "                It takes a float tensor with shape [b, n, d] and\n",
    "                returns a float tensor with the same shape.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.multihead = MultiheadAttention(d, h)\n",
    "        self.layer_norm1 = nn.LayerNorm(d)\n",
    "        self.layer_norm2 = nn.LayerNorm(d)\n",
    "        self.rff = rff\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        It is equivariant to permutations of the\n",
    "        second dimension of tensor x (`n`).\n",
    "\n",
    "        It is invariant to permutations of the\n",
    "        second dimension of tensor y (`m`).\n",
    "\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "            y: a float tensor with shape [b, m, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "        h = self.layer_norm1(x + self.multihead(x, y, y))\n",
    "        return self.layer_norm2(h + self.rff(h))\n",
    "\n",
    "class SetAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d, h, rff):\n",
    "        super().__init__()\n",
    "        self.mab = MultiheadAttentionBlock(d, h, rff)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "        return self.mab(x, x)\n",
    "\n",
    "class InducedSetAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d, m, h, rff1, rff2):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            d: an integer, input dimension.\n",
    "            m: an integer, number of inducing points.\n",
    "            h: an integer, number of heads.\n",
    "            rff1, rff2: modules, row-wise feedforward layers.\n",
    "                It takes a float tensor with shape [b, n, d] and\n",
    "                returns a float tensor with the same shape.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mab1 = MultiheadAttentionBlock(d, h, rff1)\n",
    "        self.mab2 = MultiheadAttentionBlock(d, h, rff2)\n",
    "        self.inducing_points = nn.Parameter(torch.randn(1, m, d))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, n, d].\n",
    "        \"\"\"\n",
    "        b = x.size(0)\n",
    "        p = self.inducing_points\n",
    "        p = p.repeat([b, 1, 1])  # shape [b, m, d]\n",
    "        h = self.mab1(p, x)  # shape [b, m, d]\n",
    "        return self.mab2(x, h)\n",
    "\n",
    "class PoolingMultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d, k, h, rff):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            d: an integer, input dimension.\n",
    "            k: an integer, number of seed vectors.\n",
    "            h: an integer, number of heads.\n",
    "            rff: a module, row-wise feedforward layers.\n",
    "                It takes a float tensor with shape [b, n, d] and\n",
    "                returns a float tensor with the same shape.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mab = MultiheadAttentionBlock(d, h, rff)\n",
    "        self.seed_vectors = nn.Parameter(torch.randn(1, k, d))\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            z: a float tensor with shape [b, n, d].\n",
    "        Returns:\n",
    "            a float tensor with shape [b, k, d].\n",
    "        \"\"\"\n",
    "        b = z.size(0)\n",
    "        s = self.seed_vectors\n",
    "        s = s.repeat([b, 1, 1])  # random seed vector: shape [b, k, d]\n",
    "\n",
    "        output = self.mab(s, z)\n",
    "        # print('PoolingMultiheadAttention', output.shape)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set transformer for ingredient representation\n",
    "class SetTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SetTransformer, self).__init__()\n",
    "        in_dimension = 46 # 300\n",
    "        out_dimension = 128 # 600\n",
    "\n",
    "        d = in_dimension\n",
    "        m = 16 # number of inducing points\n",
    "        h = 2  # number of heads\n",
    "        k = 2  # number of seed vectors\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            InducedSetAttentionBlock(d, m, h, RFF(d), RFF(d)),\n",
    "            InducedSetAttentionBlock(d, m, h, RFF(d), RFF(d))\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            PoolingMultiheadAttention(d, k, h, RFF(d)),\n",
    "            SetAttentionBlock(d, h, RFF(d))\n",
    "        )\n",
    "        self.decoder_2 = nn.Sequential(\n",
    "            PoolingMultiheadAttention(d, k, h, RFF(d))\n",
    "        )\n",
    "        self.decoder_3 = nn.Sequential(\n",
    "            SetAttentionBlock(d, h, RFF(d))\n",
    "        )\n",
    "\n",
    "        self.predictor = nn.Linear(k * d, out_dimension)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [batch, n, in_dimension].\n",
    "        Returns:\n",
    "            a float tensor with shape [batch, out_dimension].\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.encoder(x) # shape [batch, batch_max_len, d]\n",
    "        x = self.decoder(x) # shape [batch, k, d]\n",
    "\n",
    "        b, k, d = x.shape\n",
    "        x = x.view(b, k * d)\n",
    "        y = self.predictor(x)\n",
    "        \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## textCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textCNN(nn.Module):\n",
    "    def __init__(self, dim_channel, kernel_wins, dropout_rate, num_class):\n",
    "        super(textCNN, self).__init__()\n",
    "    \n",
    "        # Convolutional Layers with different window size kernels\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, dim_channel, (w, 1024)) for w in kernel_wins])\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc = nn.Linear(len(kernel_wins)*dim_channel, num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        con_x = [conv(x) for conv in self.convs]\n",
    "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x]\n",
    "        fc_x = torch.cat(pool_x, dim=1)\n",
    "        fc_x = fc_x.squeeze(-1)\n",
    "        fc_x = self.dropout(fc_x)\n",
    "        logit = self.fc(fc_x)\n",
    "        \n",
    "        return logit\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationAttention(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size=16):\n",
    "        super(RelationAttention, self).__init__()\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(in_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        w = self.project(z).mean(0)                    # (M, 1)\n",
    "        beta = torch.softmax(w, dim=0)                 # (M, 1)\n",
    "        beta = beta.expand((z.shape[0],) + beta.shape) # (N, M, 1)\n",
    "        out = (beta * z).sum(1)                        # (N, D * K)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, rel_names):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        self.num_heads = 8\n",
    "        self.hid_feats = int(hid_feats/self.num_heads)\n",
    "        self.out_feats = int(out_feats/self.num_heads)\n",
    "        \n",
    "        self.relation_attention = RelationAttention(in_feats) # in_feats*self.num_heads\n",
    "        \n",
    "        self.gatconv1 = dglnn.HeteroGraphConv({\n",
    "            'i-r': dglnn.GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
    "            'r-i': dglnn.GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
    "            'r-r': dglnn.GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
    "            'i-i': dglnn.GATConv(in_feats, self.hid_feats, num_heads=self.num_heads),\n",
    "            }, aggregate='stack') # sum\n",
    "        \n",
    "        self.gatconv2 = dglnn.HeteroGraphConv({\n",
    "            'i-r': dglnn.GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
    "            'r-i': dglnn.GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
    "            'r-r': dglnn.GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
    "            'i-i': dglnn.GATConv(self.hid_feats*self.num_heads, self.out_feats, num_heads=self.num_heads),\n",
    "            }, aggregate='stack') # sum\n",
    "        \n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(self.out_feats*self.num_heads, out_feats)\n",
    "        )\n",
    "        \n",
    "        self.combineSetTransformerLinear = nn.Sequential(\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "    def forward(self, blocks, inputs, total_ingre_emb):\n",
    "        edge_weight_0 = blocks[0].edata['weight']\n",
    "        edge_weight_1 = blocks[1].edata['weight']\n",
    "    \n",
    "        h = self.gatconv1(blocks[0], inputs, edge_weight_0)\n",
    "        h = {k: F.relu(v).flatten(2) for k, v in h.items()}\n",
    "        h = {k: self.relation_attention(v) for k, v in h.items()} \n",
    "\n",
    "        secondToLast_ingre = h['ingredient']\n",
    "        h = self.gatconv2(blocks[-1], h, edge_weight_1) # (h, h)\n",
    "        last_ingre_and_instr = h['recipe'].flatten(2) # [64, 2, 128]\n",
    "        \n",
    "        temp = last_ingre_and_instr[:,1,:]\n",
    "        total_ingre_emb = total_ingre_emb\n",
    "        temp = torch.cat([temp, total_ingre_emb], 1)\n",
    "        temp = self.combineSetTransformerLinear(temp)\n",
    "        combine_the_other = torch.cat([last_ingre_and_instr[:,0,:].unsqueeze(1), temp.unsqueeze(1)], 1)\n",
    "        \n",
    "        h = {'recipe':self.relation_attention(combine_the_other)}\n",
    "        # attention-head Weight matrix\n",
    "        h = {k: self.embedding(v) for k, v in h.items()}\n",
    "\n",
    "        return torch.squeeze(h['recipe']), secondToLast_ingre, last_ingre_and_instr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(input, p=1, dim=1, eps=1e-12):\n",
    "    return input / input.norm(p, dim, keepdim=True).clamp(min=eps).expand_as(input)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # user\n",
    "        self.user_embedding = nn.Sequential(\n",
    "            nn.Linear(300, 128),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # transform input embeddings\n",
    "        self.instr_embedding = nn.Sequential(\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.ingredient_embedding = nn.Sequential(\n",
    "            nn.Linear(46, 128),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "        self.setTransformer_ = SetTransformer()\n",
    "        self.gnn = GNN(128, 128, 128, graph.etypes) # 128\n",
    "        self.cnn = textCNN(128, kernel_wins=[3,4,5], dropout_rate=0.5, num_class=128)\n",
    "    \n",
    "        # output transformation\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(128, 9)\n",
    "        )\n",
    "        \n",
    "    def forward(self, graph, inputs, output_nodes):\n",
    "        instr, ingredient, ingredient_of_dst_recipe = inputs\n",
    "        \n",
    "        # instruction - textcnn\n",
    "        instr = self.cnn(instr.unsqueeze(1))\n",
    "        instr = norm(instr)\n",
    "\n",
    "        # ingredient\n",
    "        ingredient = self.ingredient_embedding(ingredient)\n",
    "        ingredient = norm(ingredient)\n",
    "\n",
    "        # for setTransformer\n",
    "        all_ingre_emb_for_each_recipe = get_ingredient_neighbors_all_embeddings(graph, output_nodes, ingredient_of_dst_recipe)\n",
    "        all_ingre_emb_for_each_recipe = norm(all_ingre_emb_for_each_recipe)\n",
    "        total_ingre_emb = self.setTransformer_(all_ingre_emb_for_each_recipe)\n",
    "        \n",
    "        # GNN\n",
    "        output, secondToLast_ingre, last_ingre_and_instr = self.gnn(graph, {'recipe': instr, 'ingredient': ingredient}, total_ingre_emb)\n",
    "        total_pos_score, total_neg_score = get_ingredient_neighbors_link_scores(graph, output_nodes, secondToLast_ingre, output)\n",
    "        \n",
    "        return self.out(output), secondToLast_ingre, output, total_pos_score, total_neg_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train_dataloader:  425\n",
      "# of batches in val_dataloader:  91\n",
      "# of batches in test_dataloader:  92\n",
      "\n",
      "blocks:  [Block(num_src_nodes={'ingredient': 3434, 'recipe': 12248},\n",
      "      num_dst_nodes={'ingredient': 448, 'recipe': 683},\n",
      "      num_edges={('ingredient', 'i-i', 'ingredient'): 7135, ('ingredient', 'i-r', 'recipe'): 4366, ('recipe', 'r-i', 'ingredient'): 8364, ('recipe', 'r-r', 'recipe'): 9527},\n",
      "      metagraph=[('ingredient', 'ingredient', 'i-i'), ('ingredient', 'recipe', 'i-r'), ('recipe', 'ingredient', 'r-i'), ('recipe', 'recipe', 'r-r')]), Block(num_src_nodes={'ingredient': 448, 'recipe': 683},\n",
      "      num_dst_nodes={'ingredient': 0, 'recipe': 128},\n",
      "      num_edges={('ingredient', 'i-i', 'ingredient'): 0, ('ingredient', 'i-r', 'recipe'): 846, ('recipe', 'r-i', 'ingredient'): 0, ('recipe', 'r-r', 'recipe'): 564},\n",
      "      metagraph=[('ingredient', 'ingredient', 'i-i'), ('ingredient', 'recipe', 'i-r'), ('recipe', 'ingredient', 'r-i'), ('recipe', 'recipe', 'r-r')])]\n"
     ]
    }
   ],
   "source": [
    "# dataloader\n",
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([{('recipe', 'r-i', 'ingredient'): 20, \n",
    "                                                     ('ingredient', 'i-r', 'recipe'): 20, \n",
    "                                                     ('recipe', 'r-r', 'recipe'): 20,\n",
    "                                                     ('ingredient', 'i-i', 'ingredient'): 20\n",
    "                                                     }]*2)\n",
    "\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, {'recipe': train_idx.cpu()}, sampler,\n",
    "    batch_size=128, shuffle=True, drop_last=False, num_workers=0)\n",
    "\n",
    "val_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, {'recipe': val_idx.cpu()}, sampler,\n",
    "    batch_size=128, shuffle=True, drop_last=False, num_workers=0)\n",
    "\n",
    "test_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, {'recipe': test_idx.cpu()}, sampler,\n",
    "    batch_size=128, shuffle=True, drop_last=False, num_workers=0)\n",
    "print('# of batches in train_dataloader: ', len(train_dataloader))\n",
    "print('# of batches in val_dataloader: ', len(val_dataloader))\n",
    "print('# of batches in test_dataloader: ', len(test_dataloader))\n",
    "print()\n",
    "\n",
    "for input_nodes, output_nodes, blocks in train_dataloader:\n",
    "    print('blocks: ', blocks)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_prediction_loss(pos_score, neg_score):\n",
    "    # Margin loss\n",
    "    n_edges = pos_score.shape[0]\n",
    "    return (1 - pos_score.unsqueeze(1) + neg_score.view(n_edges, -1)).clamp(min=0).mean()\n",
    "\n",
    "def get_score(y_pred, y_true):\n",
    "    score = {\n",
    "        \"precision\": precision_score(y_true, y_pred, labels=[1, 2, 3, 4, 5, 6, 7, 8], average='micro'), \n",
    "        \"recall\": recall_score(y_true, y_pred, labels=[1, 2, 3, 4, 5, 6, 7, 8], average='micro'),\n",
    "        \"f1\": f1_score(y_true, y_pred, labels=[1, 2, 3, 4, 5, 6, 7, 8], average='micro')\n",
    "    }\n",
    "\n",
    "    detailed_score = {\n",
    "        \"precision\": precision_score(y_true, y_pred, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8], average=None, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8], average=None, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8], average=None, zero_division=0)\n",
    "    }\n",
    "    return score, detailed_score\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    # print('evaluating ... ')\n",
    "    evaluate_start = time.time()\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    cosine_total_loss = 0\n",
    "    link_prediction_total_loss = 0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    detailed_precision = 0\n",
    "    detailed_recall = 0\n",
    "    detailed_f1 = 0\n",
    "    count = 0\n",
    "    \n",
    "    all_y_preds = None\n",
    "    all_labels = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_nodes, output_nodes, blocks in dataloader:\n",
    "\n",
    "            blocks = [blk.to(device) for blk in blocks]\n",
    "            \n",
    "            # input\n",
    "            input_instr = blocks[0].srcdata['instr_feature']['recipe']\n",
    "            input_ingredient = blocks[0].srcdata['nutrient_feature']['ingredient']\n",
    "            ingredient_of_dst_recipe = blocks[1].srcdata['nutrient_feature']['ingredient']\n",
    "\n",
    "            labels = blocks[-1].dstdata['label']['recipe']\n",
    "\n",
    "            inputs = [input_instr, input_ingredient, ingredient_of_dst_recipe]\n",
    "            logits, secondToLast_ingre, last_instr, total_pos_score, total_neg_score = model(blocks, inputs, output_nodes)\n",
    "            y_pred = np.argmax(logits.cpu(), axis=1)\n",
    "            \n",
    "            if all_y_preds is None:\n",
    "                all_y_preds = y_pred\n",
    "                all_labels = labels.cpu().numpy()\n",
    "            else:\n",
    "                all_y_preds = np.append(all_y_preds, y_pred, axis=0)\n",
    "                all_labels = np.append(all_labels, labels.cpu().numpy(), axis=0)\n",
    "            \n",
    "            # Loss\n",
    "            link_prediction_loss = get_link_prediction_loss(total_pos_score, total_neg_score)\n",
    "            loss = criterion(logits, labels) + 0.1*link_prediction_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            link_prediction_total_loss += link_prediction_loss.item()\n",
    "\n",
    "            count += len(labels)\n",
    "        \n",
    "        score, detailed_score = get_score(all_y_preds, all_labels)\n",
    "        total_precision = score['precision']\n",
    "        total_recall = score['recall']\n",
    "        total_f1 = score['f1']\n",
    "        detailed_precision = detailed_score['precision']\n",
    "        detailed_recall = detailed_score['recall']\n",
    "        detailed_f1 = detailed_score['f1']\n",
    "        \n",
    "        total_loss /= count\n",
    "        link_prediction_total_loss /= count\n",
    "        evalutate_time = time.strftime(\"%M:%S min\", time.gmtime(time.time()-evaluate_start))\n",
    "        \n",
    "    return total_loss, total_precision, total_recall, total_f1, evalutate_time, detailed_precision, detailed_recall, detailed_f1, link_prediction_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start ... \n",
      "Epoch: 0,  Loss: 1.4874,  Time: 10:53 min, LR: 0.005000\n",
      "Testing: \n",
      "Total Loss: 0.0094,  Precision: 0.6173,  Recall: 0.6348,  F1: 0.625940,  Time: 01:43 min, Link Loss: 0.0042\n",
      "detailed_precision:  [0.4435, 0.3141, 0.7973, 0.7909, 0.0, 0.4188, 0.7093, 0.4206, 0.4728]\n",
      "detailed_recall:  [0.3971, 0.0483, 0.7143, 0.6013, 0.0, 0.259, 0.9068, 0.0642, 0.9147]\n",
      "detailed_f1:  [0.419, 0.0837, 0.7535, 0.6832, 0.0, 0.3201, 0.796, 0.1114, 0.6234]\n",
      "\n",
      "Epoch: 1,  Loss: 1.1655,  Time: 09:58 min, LR: 0.004750\n",
      "Testing: \n",
      "Total Loss: 0.0083,  Precision: 0.6720,  Recall: 0.6871,  F1: 0.679463,  Time: 01:45 min, Link Loss: 0.0041\n",
      "detailed_precision:  [0.4878, 0.4684, 0.852, 0.7155, 0.2857, 0.6256, 0.7312, 0.4384, 0.6459]\n",
      "detailed_recall:  [0.4476, 0.3793, 0.7076, 0.7492, 0.0175, 0.3987, 0.9062, 0.4365, 0.7241]\n",
      "detailed_f1:  [0.4669, 0.4192, 0.7731, 0.7319, 0.0329, 0.4871, 0.8093, 0.4375, 0.6828]\n",
      "\n",
      "Epoch: 2,  Loss: 1.0598,  Time: 09:58 min, LR: 0.004513\n",
      "Testing: \n",
      "Total Loss: 0.0080,  Precision: 0.6723,  Recall: 0.7312,  F1: 0.700508,  Time: 01:44 min, Link Loss: 0.0038\n",
      "detailed_precision:  [0.5967, 0.6187, 0.7227, 0.6703, 0.3846, 0.5449, 0.7421, 0.481, 0.6631]\n",
      "detailed_recall:  [0.4042, 0.3389, 0.8571, 0.8175, 0.0218, 0.6484, 0.889, 0.5949, 0.7448]\n",
      "detailed_f1:  [0.482, 0.4379, 0.7842, 0.7366, 0.0413, 0.5921, 0.8089, 0.5319, 0.7016]\n",
      "\n",
      "Epoch: 3,  Loss: 1.0014,  Time: 09:59 min, LR: 0.004287\n",
      "Testing: \n",
      "Total Loss: 0.0078,  Precision: 0.6962,  Recall: 0.7155,  F1: 0.705700,  Time: 01:44 min, Link Loss: 0.0037\n",
      "detailed_precision:  [0.563, 0.6786, 0.8407, 0.7239, 0.2898, 0.6826, 0.7914, 0.5797, 0.5765]\n",
      "detailed_recall:  [0.5054, 0.335, 0.7597, 0.8163, 0.2227, 0.4694, 0.851, 0.4722, 0.8721]\n",
      "detailed_f1:  [0.5326, 0.4485, 0.7981, 0.7674, 0.2519, 0.5563, 0.8201, 0.5204, 0.6942]\n",
      "\n",
      "Epoch: 4,  Loss: 0.9612,  Time: 10:00 min, LR: 0.004073\n",
      "Testing: \n",
      "Total Loss: 0.0076,  Precision: 0.6912,  Recall: 0.7239,  F1: 0.707193,  Time: 01:43 min, Link Loss: 0.0035\n",
      "detailed_precision:  [0.5866, 0.686, 0.8043, 0.6349, 0.3886, 0.7194, 0.8122, 0.5807, 0.5775]\n",
      "detailed_recall:  [0.4843, 0.3961, 0.8195, 0.9037, 0.2969, 0.3783, 0.8168, 0.4979, 0.8802]\n",
      "detailed_f1:  [0.5305, 0.5022, 0.8118, 0.7458, 0.3366, 0.4959, 0.8145, 0.5361, 0.6974]\n",
      "\n",
      "Epoch: 5,  Loss: 0.9403,  Time: 09:56 min, LR: 0.003869\n",
      "Testing: \n",
      "Total Loss: 0.0075,  Precision: 0.7060,  Recall: 0.7172,  F1: 0.711581,  Time: 01:43 min, Link Loss: 0.0034\n",
      "detailed_precision:  [0.5711, 0.673, 0.7706, 0.5862, 0.4609, 0.7031, 0.8237, 0.6501, 0.6359]\n",
      "detailed_recall:  [0.5376, 0.4197, 0.8405, 0.925, 0.2576, 0.5651, 0.775, 0.4479, 0.841]\n",
      "detailed_f1:  [0.5538, 0.517, 0.804, 0.7176, 0.3305, 0.6266, 0.7986, 0.5304, 0.7242]\n",
      "\n",
      "Epoch: 6,  Loss: 0.9139,  Time: 10:06 min, LR: 0.003675\n",
      "Testing: \n",
      "Total Loss: 0.0071,  Precision: 0.7011,  Recall: 0.7614,  F1: 0.729978,  Time: 01:45 min, Link Loss: 0.0034\n",
      "detailed_precision:  [0.6486, 0.5582, 0.7759, 0.7643, 0.557, 0.609, 0.7944, 0.5632, 0.6473]\n",
      "detailed_recall:  [0.4432, 0.5576, 0.8472, 0.7917, 0.1921, 0.6797, 0.8484, 0.5977, 0.8341]\n",
      "detailed_f1:  [0.5266, 0.5579, 0.81, 0.7778, 0.2857, 0.6424, 0.8206, 0.5799, 0.7289]\n",
      "\n",
      "Epoch: 7,  Loss: 0.9024,  Time: 10:09 min, LR: 0.003492\n",
      "Testing: \n",
      "Total Loss: 0.0070,  Precision: 0.7320,  Recall: 0.7330,  F1: 0.732509,  Time: 01:46 min, Link Loss: 0.0032\n",
      "detailed_precision:  [0.5809, 0.7406, 0.7558, 0.7393, 0.4683, 0.6451, 0.8388, 0.5808, 0.6549]\n",
      "detailed_recall:  [0.5779, 0.4079, 0.8671, 0.813, 0.2576, 0.6248, 0.8184, 0.6049, 0.8122]\n",
      "detailed_f1:  [0.5794, 0.526, 0.8076, 0.7744, 0.3324, 0.6348, 0.8285, 0.5926, 0.7251]\n",
      "\n",
      "Epoch: 8,  Loss: 0.8799,  Time: 10:09 min, LR: 0.003317\n",
      "Testing: \n",
      "Total Loss: 0.0069,  Precision: 0.7384,  Recall: 0.7527,  F1: 0.745489,  Time: 01:45 min, Link Loss: 0.0032\n",
      "detailed_precision:  [0.5896, 0.6418, 0.8238, 0.7335, 0.4794, 0.7739, 0.7848, 0.5994, 0.7222]\n",
      "detailed_recall:  [0.5476, 0.4837, 0.8283, 0.8231, 0.4061, 0.4835, 0.8995, 0.6106, 0.7713]\n",
      "detailed_f1:  [0.5678, 0.5517, 0.8261, 0.7757, 0.4397, 0.5952, 0.8382, 0.6049, 0.746]\n",
      "\n",
      "Epoch: 9,  Loss: 0.8687,  Time: 09:57 min, LR: 0.003151\n",
      "Testing: \n",
      "Total Loss: 0.0068,  Precision: 0.7113,  Recall: 0.7776,  F1: 0.742987,  Time: 01:44 min, Link Loss: 0.0031\n",
      "detailed_precision:  [0.6715, 0.5833, 0.7323, 0.7523, 0.4964, 0.6075, 0.8017, 0.6036, 0.6831]\n",
      "detailed_recall:  [0.4413, 0.5724, 0.8815, 0.813, 0.2969, 0.7363, 0.8679, 0.582, 0.8197]\n",
      "detailed_f1:  [0.5326, 0.5778, 0.8, 0.7815, 0.3716, 0.6657, 0.8335, 0.5926, 0.7452]\n",
      "\n",
      "Epoch: 10,  Loss: 0.8499,  Time: 09:50 min, LR: 0.002994\n",
      "Testing: \n",
      "Total Loss: 0.0070,  Precision: 0.7344,  Recall: 0.7495,  F1: 0.741879,  Time: 01:42 min, Link Loss: 0.0031\n",
      "detailed_precision:  [0.596, 0.6902, 0.7351, 0.7803, 0.4538, 0.6068, 0.8518, 0.6, 0.6901]\n",
      "detailed_recall:  [0.5512, 0.4631, 0.8848, 0.7794, 0.4716, 0.7535, 0.8034, 0.6334, 0.8157]\n",
      "detailed_f1:  [0.5727, 0.5542, 0.803, 0.7798, 0.4625, 0.6723, 0.8269, 0.6162, 0.7476]\n",
      "\n",
      "Epoch: 11,  Loss: 0.8395,  Time: 09:47 min, LR: 0.002844\n",
      "Testing: \n",
      "Total Loss: 0.0068,  Precision: 0.7299,  Recall: 0.7553,  F1: 0.742374,  Time: 01:41 min, Link Loss: 0.0031\n",
      "detailed_precision:  [0.609, 0.6282, 0.8439, 0.731, 0.5089, 0.702, 0.8177, 0.6823, 0.6323]\n",
      "detailed_recall:  [0.5309, 0.5143, 0.8084, 0.8488, 0.3755, 0.6138, 0.859, 0.4565, 0.856]\n",
      "detailed_f1:  [0.5672, 0.5655, 0.8258, 0.7855, 0.4322, 0.6549, 0.8378, 0.547, 0.7274]\n",
      "\n",
      "Epoch: 12,  Loss: 0.8295,  Time: 09:47 min, LR: 0.002702\n",
      "Testing: \n",
      "Total Loss: 0.0068,  Precision: 0.7177,  Recall: 0.7803,  F1: 0.747656,  Time: 01:42 min, Link Loss: 0.0031\n",
      "detailed_precision:  [0.6655, 0.7636, 0.7955, 0.7676, 0.7, 0.6062, 0.7694, 0.565, 0.6602]\n",
      "detailed_recall:  [0.4516, 0.3882, 0.8571, 0.8287, 0.214, 0.7394, 0.9126, 0.6576, 0.845]\n",
      "detailed_f1:  [0.5381, 0.5147, 0.8252, 0.797, 0.3278, 0.6662, 0.8349, 0.6078, 0.7413]\n",
      "\n",
      "Epoch: 13,  Loss: 0.8201,  Time: 09:44 min, LR: 0.002567\n",
      "Epoch: 14,  Loss: 0.8108,  Time: 09:44 min, LR: 0.002438\n",
      "Testing: \n",
      "Total Loss: 0.0068,  Precision: 0.7157,  Recall: 0.7862,  F1: 0.749317,  Time: 01:40 min, Link Loss: 0.0029\n",
      "detailed_precision:  [0.6844, 0.6363, 0.8426, 0.751, 0.478, 0.5482, 0.8063, 0.5875, 0.6777]\n",
      "detailed_recall:  [0.4361, 0.5153, 0.8184, 0.841, 0.4279, 0.8305, 0.8752, 0.6419, 0.8283]\n",
      "detailed_f1:  [0.5327, 0.5694, 0.8303, 0.7934, 0.4516, 0.6604, 0.8394, 0.6135, 0.7455]\n",
      "\n",
      "Epoch: 15,  Loss: 0.7956,  Time: 09:45 min, LR: 0.002316\n",
      "Testing: \n",
      "Total Loss: 0.0067,  Precision: 0.7425,  Recall: 0.7628,  F1: 0.752507,  Time: 01:42 min, Link Loss: 0.0029\n",
      "detailed_precision:  [0.6153, 0.6897, 0.8176, 0.7285, 0.514, 0.6593, 0.8466, 0.5884, 0.6855]\n",
      "detailed_recall:  [0.5536, 0.4818, 0.8538, 0.8746, 0.4803, 0.6531, 0.8328, 0.6505, 0.8185]\n",
      "detailed_f1:  [0.5828, 0.5673, 0.8353, 0.7949, 0.4966, 0.6562, 0.8396, 0.6179, 0.7461]\n",
      "\n",
      "Epoch: 16,  Loss: 0.7897,  Time: 09:49 min, LR: 0.002201\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7269,  Recall: 0.7798,  F1: 0.752465,  Time: 01:43 min, Link Loss: 0.0029\n",
      "detailed_precision:  [0.6551, 0.6577, 0.8018, 0.777, 0.5153, 0.7087, 0.785, 0.5965, 0.6682]\n",
      "detailed_recall:  [0.4795, 0.5074, 0.8738, 0.7962, 0.441, 0.5651, 0.9062, 0.6305, 0.8376]\n",
      "detailed_f1:  [0.5537, 0.5729, 0.8362, 0.7865, 0.4753, 0.6288, 0.8412, 0.613, 0.7434]\n",
      "\n",
      "Epoch: 17,  Loss: 0.7819,  Time: 09:59 min, LR: 0.002091\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7537,  Recall: 0.7550,  F1: 0.754322,  Time: 01:44 min, Link Loss: 0.0028\n",
      "detailed_precision:  [0.5916, 0.7014, 0.8741, 0.7711, 0.75, 0.6545, 0.8092, 0.6835, 0.6765]\n",
      "detailed_recall:  [0.5878, 0.5044, 0.7918, 0.8298, 0.1703, 0.7017, 0.8784, 0.5421, 0.8036]\n",
      "detailed_f1:  [0.5897, 0.5868, 0.8309, 0.7994, 0.2776, 0.6773, 0.8424, 0.6046, 0.7346]\n",
      "\n",
      "Epoch: 18,  Loss: 0.7737,  Time: 10:00 min, LR: 0.001986\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7422,  Recall: 0.7646,  F1: 0.753236,  Time: 01:41 min, Link Loss: 0.0028\n",
      "detailed_precision:  [0.6272, 0.7218, 0.8377, 0.7322, 0.505, 0.6569, 0.8387, 0.6698, 0.6442]\n",
      "detailed_recall:  [0.5575, 0.4601, 0.8461, 0.8544, 0.4454, 0.7394, 0.8542, 0.5036, 0.8491]\n",
      "detailed_f1:  [0.5903, 0.562, 0.8419, 0.7886, 0.4733, 0.6957, 0.8463, 0.5749, 0.7326]\n",
      "\n",
      "Epoch: 19,  Loss: 0.7669,  Time: 09:58 min, LR: 0.001887\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7443,  Recall: 0.7637,  F1: 0.753869,  Time: 01:44 min, Link Loss: 0.0028\n",
      "detailed_precision:  [0.6135, 0.6637, 0.863, 0.7231, 0.6795, 0.6996, 0.8206, 0.6458, 0.6664]\n",
      "detailed_recall:  [0.5544, 0.5232, 0.7951, 0.8802, 0.2314, 0.5997, 0.8653, 0.5749, 0.8514]\n",
      "detailed_f1:  [0.5824, 0.5851, 0.8277, 0.7939, 0.3453, 0.6458, 0.8424, 0.6083, 0.7476]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20,  Loss: 0.7554,  Time: 10:01 min, LR: 0.001792\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7401,  Recall: 0.7820,  F1: 0.760463,  Time: 01:44 min, Link Loss: 0.0028\n",
      "detailed_precision:  [0.6351, 0.6352, 0.8392, 0.7615, 0.6618, 0.6991, 0.7792, 0.678, 0.6958]\n",
      "detailed_recall:  [0.5026, 0.5695, 0.8494, 0.8331, 0.393, 0.5871, 0.91, 0.5407, 0.8341]\n",
      "detailed_f1:  [0.5611, 0.6005, 0.8442, 0.7957, 0.4932, 0.6382, 0.8396, 0.6016, 0.7587]\n",
      "\n",
      "Epoch: 21,  Loss: 0.7509,  Time: 09:47 min, LR: 0.001703\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7314,  Recall: 0.7846,  F1: 0.757056,  Time: 01:41 min, Link Loss: 0.0028\n",
      "detailed_precision:  [0.6736, 0.6856, 0.8244, 0.7325, 0.4067, 0.6672, 0.7953, 0.6425, 0.7024]\n",
      "detailed_recall:  [0.493, 0.5113, 0.8527, 0.8555, 0.6376, 0.6954, 0.8899, 0.6334, 0.7955]\n",
      "detailed_f1:  [0.5693, 0.5858, 0.8383, 0.7893, 0.4966, 0.681, 0.8399, 0.6379, 0.7461]\n",
      "\n",
      "Epoch: 22,  Loss: 0.7438,  Time: 09:47 min, LR: 0.001618\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7393,  Recall: 0.7729,  F1: 0.755762,  Time: 01:42 min, Link Loss: 0.0027\n",
      "detailed_precision:  [0.6437, 0.6395, 0.8684, 0.727, 0.4167, 0.6431, 0.8288, 0.7033, 0.696]\n",
      "detailed_recall:  [0.536, 0.5645, 0.8184, 0.8589, 0.6332, 0.7582, 0.8433, 0.5107, 0.8295]\n",
      "detailed_f1:  [0.585, 0.5997, 0.8426, 0.7875, 0.5026, 0.696, 0.836, 0.5917, 0.7569]\n",
      "\n",
      "Epoch: 23,  Loss: 0.7386,  Time: 09:52 min, LR: 0.001537\n",
      "Testing: \n",
      "Total Loss: 0.0065,  Precision: 0.7450,  Recall: 0.7841,  F1: 0.764027,  Time: 01:42 min, Link Loss: 0.0027\n",
      "detailed_precision:  [0.6658, 0.7694, 0.8494, 0.71, 0.5912, 0.6276, 0.8219, 0.6334, 0.6839]\n",
      "detailed_recall:  [0.5372, 0.4798, 0.8494, 0.8802, 0.4105, 0.7724, 0.8685, 0.6434, 0.8364]\n",
      "detailed_f1:  [0.5947, 0.591, 0.8494, 0.786, 0.4845, 0.6925, 0.8446, 0.6384, 0.7525]\n",
      "\n",
      "Epoch: 24,  Loss: 0.7306,  Time: 09:59 min, LR: 0.001460\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7429,  Recall: 0.7725,  F1: 0.757381,  Time: 01:44 min, Link Loss: 0.0027\n",
      "detailed_precision:  [0.6354, 0.5786, 0.8748, 0.7281, 0.5944, 0.7003, 0.8198, 0.6523, 0.7077]\n",
      "detailed_recall:  [0.542, 0.6128, 0.8051, 0.8544, 0.3712, 0.675, 0.8682, 0.5835, 0.799]\n",
      "detailed_f1:  [0.585, 0.5952, 0.8385, 0.7862, 0.457, 0.6875, 0.8433, 0.616, 0.7505]\n",
      "\n",
      "Epoch: 25,  Loss: 0.7242,  Time: 09:52 min, LR: 0.001387\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7296,  Recall: 0.7931,  F1: 0.760025,  Time: 01:42 min, Link Loss: 0.0027\n",
      "detailed_precision:  [0.6833, 0.6487, 0.8457, 0.7706, 0.4053, 0.6904, 0.7877, 0.627, 0.7006]\n",
      "detailed_recall:  [0.464, 0.5586, 0.8439, 0.8387, 0.6725, 0.686, 0.8973, 0.6619, 0.8007]\n",
      "detailed_f1:  [0.5527, 0.6003, 0.8448, 0.8032, 0.5057, 0.6882, 0.8389, 0.644, 0.7473]\n",
      "\n",
      "Epoch: 26,  Loss: 0.7160,  Time: 09:48 min, LR: 0.001318\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7618,  Recall: 0.7598,  F1: 0.760827,  Time: 01:42 min, Link Loss: 0.0027\n",
      "detailed_precision:  [0.5988, 0.7396, 0.848, 0.7242, 0.5374, 0.6336, 0.8423, 0.6309, 0.7401]\n",
      "detailed_recall:  [0.6045, 0.4729, 0.8339, 0.8735, 0.5022, 0.752, 0.8472, 0.6291, 0.7627]\n",
      "detailed_f1:  [0.6017, 0.5769, 0.8409, 0.7919, 0.5192, 0.6877, 0.8447, 0.63, 0.7512]\n",
      "\n",
      "Epoch: 27,  Loss: 0.7124,  Time: 09:47 min, LR: 0.001252\n",
      "Testing: \n",
      "Total Loss: 0.0065,  Precision: 0.7523,  Recall: 0.7675,  F1: 0.759835,  Time: 01:42 min, Link Loss: 0.0026\n",
      "detailed_precision:  [0.6256, 0.6543, 0.8362, 0.712, 0.544, 0.6862, 0.829, 0.6469, 0.7285]\n",
      "detailed_recall:  [0.5791, 0.5389, 0.8538, 0.897, 0.4585, 0.697, 0.8491, 0.6063, 0.7742]\n",
      "detailed_f1:  [0.6014, 0.591, 0.8449, 0.7939, 0.4976, 0.6916, 0.8389, 0.6259, 0.7506]\n",
      "\n",
      "Epoch: 28,  Loss: 0.7062,  Time: 09:48 min, LR: 0.001189\n",
      "Testing: \n",
      "Total Loss: 0.0065,  Precision: 0.7405,  Recall: 0.7878,  F1: 0.763452,  Time: 01:42 min, Link Loss: 0.0026\n",
      "detailed_precision:  [0.6745, 0.6679, 0.8403, 0.7442, 0.5236, 0.6126, 0.816, 0.6584, 0.7001]\n",
      "detailed_recall:  [0.5157, 0.5369, 0.845, 0.86, 0.4847, 0.8116, 0.8746, 0.6049, 0.8162]\n",
      "detailed_f1:  [0.5845, 0.5953, 0.8426, 0.7979, 0.5034, 0.6982, 0.8443, 0.6305, 0.7537]\n",
      "\n",
      "Epoch: 29,  Loss: 0.7032,  Time: 09:48 min, LR: 0.001130\n",
      "Testing: \n",
      "Total Loss: 0.0065,  Precision: 0.7363,  Recall: 0.7856,  F1: 0.760136,  Time: 01:42 min, Link Loss: 0.0026\n",
      "detailed_precision:  [0.6681, 0.6738, 0.852, 0.7331, 0.5294, 0.6716, 0.8171, 0.5901, 0.6822]\n",
      "detailed_recall:  [0.5034, 0.5251, 0.8416, 0.8645, 0.4716, 0.7127, 0.8653, 0.6776, 0.8358]\n",
      "detailed_f1:  [0.5742, 0.5903, 0.8468, 0.7934, 0.4988, 0.6915, 0.8405, 0.6308, 0.7512]\n",
      "\n",
      "Epoch: 30,  Loss: 0.6950,  Time: 09:49 min, LR: 0.001073\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7473,  Recall: 0.7763,  F1: 0.761536,  Time: 01:42 min, Link Loss: 0.0026\n",
      "detailed_precision:  [0.6359, 0.734, 0.879, 0.7398, 0.5488, 0.6632, 0.7989, 0.681, 0.6833]\n",
      "detailed_recall:  [0.5452, 0.4867, 0.8128, 0.8499, 0.5153, 0.6954, 0.8896, 0.5421, 0.8427]\n",
      "detailed_f1:  [0.587, 0.5853, 0.8446, 0.791, 0.5315, 0.6789, 0.8418, 0.6037, 0.7547]\n",
      "\n",
      "Epoch: 31,  Loss: 0.6924,  Time: 09:58 min, LR: 0.001020\n",
      "Testing: \n",
      "Total Loss: 0.0065,  Precision: 0.7485,  Recall: 0.7918,  F1: 0.769546,  Time: 01:44 min, Link Loss: 0.0026\n",
      "detailed_precision:  [0.6714, 0.6602, 0.8368, 0.7367, 0.6205, 0.6561, 0.8185, 0.6515, 0.7112]\n",
      "detailed_recall:  [0.5281, 0.5665, 0.8461, 0.8712, 0.4498, 0.7159, 0.8803, 0.6348, 0.8312]\n",
      "detailed_f1:  [0.5912, 0.6098, 0.8414, 0.7984, 0.5215, 0.6847, 0.8483, 0.6431, 0.7665]\n",
      "\n",
      "Epoch: 32,  Loss: 0.6854,  Time: 10:00 min, LR: 0.000969\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7455,  Recall: 0.7770,  F1: 0.760947,  Time: 01:42 min, Link Loss: 0.0026\n",
      "detailed_precision:  [0.6429, 0.6869, 0.8526, 0.7253, 0.5858, 0.6307, 0.8144, 0.6711, 0.6974]\n",
      "detailed_recall:  [0.5428, 0.534, 0.8261, 0.8634, 0.4323, 0.7535, 0.8708, 0.5763, 0.8151]\n",
      "detailed_f1:  [0.5886, 0.6009, 0.8391, 0.7883, 0.4975, 0.6867, 0.8416, 0.6201, 0.7517]\n",
      "\n",
      "Epoch: 33,  Loss: 0.6798,  Time: 09:45 min, LR: 0.000920\n",
      "Testing: \n",
      "Total Loss: 0.0065,  Precision: 0.7377,  Recall: 0.7844,  F1: 0.760338,  Time: 01:41 min, Link Loss: 0.0026\n",
      "detailed_precision:  [0.6703, 0.6937, 0.856, 0.7178, 0.4887, 0.638, 0.8192, 0.657, 0.6785]\n",
      "detailed_recall:  [0.5141, 0.5064, 0.8295, 0.8858, 0.5677, 0.7221, 0.866, 0.612, 0.845]\n",
      "detailed_f1:  [0.5819, 0.5854, 0.8425, 0.793, 0.5253, 0.6775, 0.8419, 0.6337, 0.7527]\n",
      "\n",
      "Epoch: 34,  Loss: 0.6829,  Time: 09:51 min, LR: 0.000874\n",
      "Testing: \n",
      "Total Loss: 0.0064,  Precision: 0.7543,  Recall: 0.7841,  F1: 0.768888,  Time: 01:40 min, Link Loss: 0.0026\n",
      "detailed_precision:  [0.6528, 0.6671, 0.8597, 0.7598, 0.5467, 0.6731, 0.8184, 0.6704, 0.7101]\n",
      "detailed_recall:  [0.5579, 0.5626, 0.8416, 0.8466, 0.5371, 0.7143, 0.8842, 0.6006, 0.803]\n",
      "detailed_f1:  [0.6017, 0.6104, 0.8506, 0.8008, 0.5419, 0.6931, 0.85, 0.6336, 0.7537]\n",
      "\n",
      "Epoch: 35,  Loss: 0.6737,  Time: 09:58 min, LR: 0.000830\n",
      "Testing: \n",
      "Total Loss: 0.0065,  Precision: 0.7426,  Recall: 0.7865,  F1: 0.763955,  Time: 01:41 min, Link Loss: 0.0026\n",
      "detailed_precision:  [0.6645, 0.6367, 0.8524, 0.7483, 0.5851, 0.6071, 0.8327, 0.6581, 0.6987]\n",
      "detailed_recall:  [0.5197, 0.5852, 0.8317, 0.8488, 0.4803, 0.81, 0.8497, 0.6205, 0.8335]\n",
      "detailed_f1:  [0.5832, 0.6099, 0.8419, 0.7954, 0.5276, 0.694, 0.8411, 0.6388, 0.7602]\n",
      "\n",
      "Epoch: 36,  Loss: 0.6685,  Time: 09:57 min, LR: 0.000789\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7518,  Recall: 0.7749,  F1: 0.763152,  Time: 01:41 min, Link Loss: 0.0026\n",
      "detailed_precision:  [0.6318, 0.7279, 0.8486, 0.755, 0.6, 0.6344, 0.8233, 0.6562, 0.6915]\n",
      "detailed_recall:  [0.5603, 0.4877, 0.8317, 0.8387, 0.4978, 0.7818, 0.8622, 0.5991, 0.8278]\n",
      "detailed_f1:  [0.5939, 0.5841, 0.84, 0.7947, 0.5442, 0.7004, 0.8423, 0.6264, 0.7535]\n",
      "\n",
      "Epoch: 37,  Loss: 0.6624,  Time: 10:01 min, LR: 0.000749\n",
      "Testing: \n",
      "Total Loss: 0.0067,  Precision: 0.7447,  Recall: 0.7778,  F1: 0.760882,  Time: 01:44 min, Link Loss: 0.0026\n",
      "detailed_precision:  [0.6538, 0.7121, 0.8366, 0.7068, 0.534, 0.6727, 0.8242, 0.6636, 0.6821]\n",
      "detailed_recall:  [0.5468, 0.4995, 0.8394, 0.888, 0.4803, 0.697, 0.859, 0.6134, 0.8404]\n",
      "detailed_f1:  [0.5955, 0.5871, 0.838, 0.7871, 0.5057, 0.6847, 0.8413, 0.6375, 0.753]\n",
      "\n",
      "Epoch: 38,  Loss: 0.6608,  Time: 09:56 min, LR: 0.000712\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7549,  Recall: 0.7677,  F1: 0.761272,  Time: 01:42 min, Link Loss: 0.0026\n",
      "detailed_precision:  [0.6172, 0.6949, 0.8646, 0.7074, 0.7099, 0.6667, 0.8439, 0.6582, 0.6878]\n",
      "detailed_recall:  [0.5787, 0.5251, 0.8272, 0.8936, 0.4061, 0.6939, 0.8312, 0.6262, 0.8312]\n",
      "detailed_f1:  [0.5973, 0.5982, 0.8455, 0.7897, 0.5167, 0.68, 0.8375, 0.6418, 0.7527]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39,  Loss: 0.6592,  Time: 09:51 min, LR: 0.000676\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7417,  Recall: 0.7885,  F1: 0.764361,  Time: 01:42 min, Link Loss: 0.0025\n",
      "detailed_precision:  [0.6746, 0.6854, 0.8305, 0.7059, 0.5607, 0.6625, 0.8119, 0.6807, 0.6947]\n",
      "detailed_recall:  [0.5177, 0.5172, 0.8571, 0.8925, 0.5852, 0.7425, 0.8692, 0.5991, 0.8324]\n",
      "detailed_f1:  [0.5858, 0.5896, 0.8436, 0.7883, 0.5726, 0.7002, 0.8396, 0.6373, 0.7573]\n",
      "\n",
      "Epoch: 40,  Loss: 0.6524,  Time: 09:51 min, LR: 0.000643\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7512,  Recall: 0.7752,  F1: 0.763025,  Time: 01:42 min, Link Loss: 0.0025\n",
      "detailed_precision:  [0.6358, 0.6716, 0.8438, 0.7373, 0.6257, 0.6878, 0.8214, 0.6569, 0.6938]\n",
      "detailed_recall:  [0.5611, 0.534, 0.8372, 0.8645, 0.4672, 0.6813, 0.8714, 0.5763, 0.8197]\n",
      "detailed_f1:  [0.5961, 0.595, 0.8405, 0.7959, 0.535, 0.6845, 0.8456, 0.614, 0.7515]\n",
      "\n",
      "Epoch: 41,  Loss: 0.6489,  Time: 09:49 min, LR: 0.000610\n",
      "Testing: \n",
      "Total Loss: 0.0067,  Precision: 0.7412,  Recall: 0.7872,  F1: 0.763503,  Time: 01:42 min, Link Loss: 0.0025\n",
      "detailed_precision:  [0.6655, 0.6294, 0.8592, 0.7207, 0.6531, 0.6263, 0.8124, 0.6372, 0.7202]\n",
      "detailed_recall:  [0.5133, 0.5773, 0.8106, 0.8813, 0.4192, 0.7786, 0.8803, 0.6262, 0.7978]\n",
      "detailed_f1:  [0.5796, 0.6023, 0.8342, 0.7929, 0.5106, 0.6942, 0.845, 0.6317, 0.757]\n",
      "\n",
      "Epoch: 42,  Loss: 0.6473,  Time: 09:50 min, LR: 0.000580\n",
      "Testing: \n",
      "Total Loss: 0.0065,  Precision: 0.7511,  Recall: 0.7792,  F1: 0.764887,  Time: 01:42 min, Link Loss: 0.0025\n",
      "detailed_precision:  [0.6379, 0.6554, 0.8403, 0.7485, 0.6258, 0.6743, 0.8068, 0.6597, 0.722]\n",
      "detailed_recall:  [0.55, 0.5547, 0.8272, 0.8567, 0.4454, 0.7378, 0.8861, 0.5863, 0.7897]\n",
      "detailed_f1:  [0.5907, 0.6009, 0.8337, 0.799, 0.5204, 0.7046, 0.8446, 0.6208, 0.7543]\n",
      "\n",
      "Epoch: 43,  Loss: 0.6408,  Time: 09:50 min, LR: 0.000551\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7462,  Recall: 0.7889,  F1: 0.766950,  Time: 01:43 min, Link Loss: 0.0025\n",
      "detailed_precision:  [0.6689, 0.6823, 0.8454, 0.7406, 0.5684, 0.6129, 0.8187, 0.6317, 0.7215]\n",
      "detailed_recall:  [0.5277, 0.5438, 0.8538, 0.8634, 0.4716, 0.8053, 0.8733, 0.6534, 0.7984]\n",
      "detailed_f1:  [0.5899, 0.6053, 0.8496, 0.7973, 0.5155, 0.6961, 0.8451, 0.6424, 0.758]\n",
      "\n",
      "Epoch: 44,  Loss: 0.6423,  Time: 09:53 min, LR: 0.000523\n",
      "Testing: \n",
      "Total Loss: 0.0066,  Precision: 0.7445,  Recall: 0.7845,  F1: 0.763966,  Time: 01:44 min, Link Loss: 0.0025\n",
      "detailed_precision:  [0.6564, 0.646, 0.8533, 0.7435, 0.5145, 0.6676, 0.8182, 0.6535, 0.7017]\n",
      "detailed_recall:  [0.5265, 0.5557, 0.8372, 0.8634, 0.5415, 0.7127, 0.8717, 0.6377, 0.8105]\n",
      "detailed_f1:  [0.5843, 0.5975, 0.8452, 0.799, 0.5277, 0.6894, 0.8441, 0.6455, 0.7522]\n",
      "\n",
      "Epoch: 45,  Loss: 0.6356,  Time: 10:03 min, LR: 0.000497\n",
      "Testing: \n",
      "Total Loss: 0.0067,  Precision: 0.7504,  Recall: 0.7880,  F1: 0.768711,  Time: 01:44 min, Link Loss: 0.0025\n",
      "detailed_precision:  [0.6587, 0.7283, 0.8612, 0.7303, 0.6085, 0.6442, 0.8039, 0.6539, 0.7132]\n",
      "detailed_recall:  [0.5372, 0.4887, 0.845, 0.8735, 0.5022, 0.7786, 0.8896, 0.6334, 0.8093]\n",
      "detailed_f1:  [0.5918, 0.5849, 0.853, 0.7955, 0.5502, 0.705, 0.8446, 0.6435, 0.7582]\n",
      "\n",
      "Epoch: 46,  Loss: 0.6334,  Time: 10:05 min, LR: 0.000472\n",
      "Testing: \n",
      "Total Loss: 0.0067,  Precision: 0.7484,  Recall: 0.7883,  F1: 0.767813,  Time: 01:46 min, Link Loss: 0.0025\n",
      "detailed_precision:  [0.662, 0.7098, 0.8719, 0.7384, 0.5269, 0.6481, 0.7995, 0.6306, 0.7378]\n",
      "detailed_recall:  [0.5321, 0.5133, 0.8062, 0.8757, 0.6419, 0.7692, 0.8969, 0.6648, 0.7748]\n",
      "detailed_f1:  [0.59, 0.5958, 0.8377, 0.8012, 0.5787, 0.7035, 0.8454, 0.6472, 0.7558]\n",
      "\n",
      "Epoch: 47,  Loss: 0.6315,  Time: 09:45 min, LR: 0.000449\n",
      "Testing: \n",
      "Total Loss: 0.0067,  Precision: 0.7431,  Recall: 0.7853,  F1: 0.763577,  Time: 01:41 min, Link Loss: 0.0025\n",
      "detailed_precision:  [0.6616, 0.7053, 0.8403, 0.6979, 0.5339, 0.6498, 0.8199, 0.6482, 0.7046]\n",
      "detailed_recall:  [0.5233, 0.5281, 0.8505, 0.9003, 0.5852, 0.7457, 0.8615, 0.6177, 0.8134]\n",
      "detailed_f1:  [0.5844, 0.6039, 0.8453, 0.7863, 0.5583, 0.6944, 0.8402, 0.6326, 0.7551]\n",
      "\n",
      "Epoch: 48,  Loss: 0.6302,  Time: 09:46 min, LR: 0.000426\n",
      "Testing: \n",
      "Total Loss: 0.0067,  Precision: 0.7469,  Recall: 0.7895,  F1: 0.767597,  Time: 01:41 min, Link Loss: 0.0025\n",
      "detailed_precision:  [0.6633, 0.6867, 0.8235, 0.7323, 0.5677, 0.6756, 0.8162, 0.6651, 0.6984]\n",
      "detailed_recall:  [0.5241, 0.5291, 0.8782, 0.8791, 0.5677, 0.7488, 0.8714, 0.6177, 0.8151]\n",
      "detailed_f1:  [0.5855, 0.5977, 0.8499, 0.799, 0.5677, 0.7103, 0.8429, 0.6405, 0.7523]\n",
      "\n",
      "Epoch: 49,  Loss: 0.6264,  Time: 09:45 min, LR: 0.000405\n",
      "Testing: \n",
      "Total Loss: 0.0067,  Precision: 0.7412,  Recall: 0.7848,  F1: 0.762395,  Time: 01:41 min, Link Loss: 0.0025\n",
      "detailed_precision:  [0.6629, 0.6596, 0.86, 0.7403, 0.566, 0.6579, 0.8027, 0.6558, 0.6961]\n",
      "detailed_recall:  [0.5193, 0.5557, 0.8228, 0.8555, 0.5808, 0.7033, 0.8816, 0.5735, 0.8301]\n",
      "detailed_f1:  [0.5824, 0.6032, 0.841, 0.7938, 0.5733, 0.6798, 0.8403, 0.6119, 0.7572]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.95)\n",
    "\n",
    "weights_class = torch.Tensor(9).fill_(1)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_class).to(device)\n",
    "criterion_cosine = torch.nn.CosineEmbeddingLoss(margin=0.1)\n",
    "\n",
    "print('start ... ')\n",
    "for epoch in range(50):\n",
    "    train_start = time.time()\n",
    "    epoch_loss = 0\n",
    "    cosine_epoch_loss = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f1 = 0\n",
    "    iteration_cnt = 0\n",
    "        \n",
    "    for batch, (input_nodes, output_nodes, blocks) in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        blocks = [b.to(device) for b in blocks]\n",
    "\n",
    "        # input\n",
    "        input_instr = blocks[0].srcdata['instr_feature']['recipe'] \n",
    "        input_ingredient = blocks[0].srcdata['nutrient_feature']['ingredient']\n",
    "        ingredient_of_dst_recipe = blocks[1].srcdata['nutrient_feature']['ingredient']\n",
    "        labels = blocks[-1].dstdata['label']['recipe'] \n",
    "\n",
    "        inputs = [input_instr, input_ingredient, ingredient_of_dst_recipe]\n",
    "        logits, secondToLast_ingre, last_instr, total_pos_score, total_neg_score = model(blocks, inputs, output_nodes)\n",
    "\n",
    "        # training scores\n",
    "        y_pred = np.argmax(logits.detach().cpu(), axis=1)\n",
    "    \n",
    "        # compute loss\n",
    "        link_prediction_loss = get_link_prediction_loss(total_pos_score, total_neg_score)\n",
    "        loss = criterion(logits, labels) + 0.1*link_prediction_loss\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        iteration_cnt += 1 \n",
    "        # break\n",
    "        \n",
    "    epoch_loss /= iteration_cnt\n",
    "    cosine_epoch_loss /= iteration_cnt\n",
    "    train_end = time.strftime(\"%M:%S min\", time.gmtime(time.time()-train_start))\n",
    "    \n",
    "    print('Epoch: {0},  Loss: {l:.4f},  Time: {t}, LR: {lr:.6f}'\n",
    "          .format(epoch, l=epoch_loss, t=train_end, lr=opt.param_groups[0]['lr']))\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluation\n",
    "    # For demonstration purpose, only test set result is reported here. Please use val_dataloader for comprehensiveness.\n",
    "    test_loss, test_precision, test_recall, test_f1, test_time, test_detailed_precision, test_detailed_recall, test_detailed_f1, link_prediction_test_loss \\\n",
    "    = evaluate(model, test_dataloader, device)\n",
    "    print('Testing: ')\n",
    "    print('Total Loss: {l:.4f},  Precision: {precision:.4f},  Recall: {recall:.4f},  F1: {f1:.6f},  Time: {t}, Link Loss: {link_loss:.4f}'\n",
    "          .format(l=test_loss, precision=test_precision, recall=test_recall, f1=test_f1, t=test_time, link_loss=link_prediction_test_loss))\n",
    "    print('detailed_precision: ', [float('{:.4f}'.format(i)) for i in list(test_detailed_precision)])\n",
    "    print('detailed_recall: ', [float('{:.4f}'.format(i)) for i in list(test_detailed_recall)])\n",
    "    print('detailed_f1: ', [float('{:.4f}'.format(i)) for i in list(test_detailed_f1)])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
